<!-- navbar -->
<nav id="mainNav" class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
  <a class="navbar-brand" href="/#page-top">mg-space</a>
  <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse navbar-right" id="navbarColor02">
    <ul class="nav navbar-nav ml-auto">
      <li class="nav-item active">
        <a class="nav-link" href="/#page-top">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/bio">Bio</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/projects">Projects</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/interests">Interests</a>
      </li>
    </ul>
    <!-- <form class="form-inline my-2 my-lg-0">
      <input class="form-control mr-sm-2" placeholder="Search" type="text">
      <button class="btn btn-secondary my-2 my-sm-0" type="submit">Search</button>
    </form> -->
  </div>
</nav>

<!-- Intro Header -->
<section id="masthead" class="content-section">
  <header>
    <div class="container">
      <h1>welcome<span id="underscore">_</span></h1>

      <div class="row">
        <div class="col-lg-4">
          <img class="profile-pic" src="images/profile-pic-b-0-1200.jpg" width="100%"></img>
        </div>

        <div class="col-lg-8">
          <div class="title-bar-index pad-content">
            <p>
              Hello! My name is Matt Gottsacker.
              I am a third-year Computer Science PhD student at the University of Central Florida. 
              I do research in the <a href="https://sreal.ucf.edu/">SREAL Lab</a> advised by <a href="https://sreal.ucf.edu/people/welch/">Prof. Greg Welch</a>.
              My research interests include virtual/augmented reality, 3D user interfaces, collaboration, and embodiment.
              I take an interdisciplinary approach to research, drawing mostly on philosophy, psychology, and media studies to both inspire research questions and design experiments to understand related phenomena.
              I design and create software and hardware to investigate these questions.
              I collect and analyze both quantitative and qualitative data to get us closer to answering them.
              {{!-- I research cross-reality interactions, or interactions between people who are on different points of a Reality-Virtuality Continuum.  --}}
              {{!-- We research how humans interact with augmented, mixed, and virtual reality technologies.  --}}
              {{!-- My particular interests lie in understanding how these technologies can interact with the traditional boundaries with which we frame physical reality rather than merely mimicking them.  --}}
              {{!-- More concretely, I have tested different methods of representing a non-VR person into a VR user's virtual environment during an interruption.  --}}
              {{!-- I am also examining this interaction from the non-VR person's perspective.  --}}
              {{!-- You can read more about that and some of my other work on the <a href="/projects">projects page</a>. --}}
            </p>
            <p>
              I made a <a href="docs/comic-book-all-compress.pdf">comic book</a> that functions like a personal statement and describes how my interests in the humanities and computers have come together into my current academic studies in human-computer interaction and virtual reality.
              I generated all the images with OpenAI's <a href="https://openai.com/dall-e-2/">DALL-E 2</a> except for some photos of the VR hardware on the last page.
            </p>

            <p>
              Some profiles elsewhere on the web:
            </p>
            <p>
              [ <a href="https://scholar.google.com/citations?user=lHM2NP4AAAAJ&hl=en">Google Scholar</a> ]
              [ <a href="docs/Curriculum_Vitae_MNG.pdf">Curriculum Vitae</a> ]
              [ <a href="https://www.linkedin.com/in/matthewgottsacker/">LinkedIn</a> ]  
              {{!-- [ <a href="https://github.com/mott-lab/">Github</a> ] --}}
            </p>
            <p>
              Email: gottsacker [at] knights.ucf.edu
            </p>
          </div>
        </div>

        
      </div>
    </div>
  </header>
</section>

<section class="content-section">
  <div class="container">
    <div class="title-bar-index pad-content">
      <h2>
        news
      </h2>
      <p>
        <ul>
          <li>
            September 2022: Conference paper accepted at ACM Symposium on Virtual Reality Software and Technology. "Effects of Environmental Noise Levels on Patient Handoff Communication." pp. 1-10. <a href="https://sreal.ucf.edu/wp-content/uploads/2022/10/main.pdf">Link to pre-print</a>.
          </li>
          <li>
            August 2022: Workshop paper accepted at IEEE International Symposium on Mixed and Augmented Reality (ISMAR). "Towards a Desktop&ndash;AR Prototyping Framework: Prototyping Cross-Reality Between Desktops and Augmented Reality." pp. 1-8. <a href="https://sreal.ucf.edu/wp-content/uploads/2022/10/ISMAR2022_Workshop_on_Prototyping_Cross_Reality_Systems.pdf">Link to pre-print</a>.
          </li>
          <li>
            August 2022: Poster accepted at IEEE ISMAR. "Exploring Cues and Signaling to Improve Cross-Reality Interruptions." pp. 1-6. <a href="https://sreal.ucf.edu/wp-content/uploads/2022/09/ISMAR22_CrossReality_camready_3.pdf">Link to pre-print</a>.
          </li>
          <li>
            Summer 2022: I started a summer research visit at the Computer Graphics and User Interfaces Lab at Columbia University. I worked with Dr. Steve Feiner on a collaborative mixed reality application. We have researched user interface challenges that arise when co-located AR users share and transition among each other's perspectives. Work is ongoing.
          </li>
          <li>
            February 2022: I was named a finalist for the <a href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">Meta PhD Fellowship</a>.
          </li>
        </ul>
      </p>
    </div>
  </div>
</section>

<section class="content-section">
  <div class="container">
    <div class="title-bar-index pad-content">
      <h2>
        research highlights
      </h2>
      <div class="row">
        <div class="col-md-12">
          <p>
            My PhD research focuses on computer-mediated interactions and transitions across different realities, or cross-reality interactions/transitions.
            This is most easily illustrated using Milgram and Kishino's Reality-Virtuality (RV) Continuum, a seminal piece of virtual reality (VR) research that introduced the figure below to classify immersive virtual systems.
            On one end of the continuum is the physical world with no virtual content displayed in it.
            On the other end is a fully virtual environment (i.e., VR).
            These extremes are set on a continuum because they can be mixed in interesting ways.
            If your primary environment is the physical world, and you add virtual content to it, you get augmented reality (AR) (think: Pokemon Go).
            If your primary environment is a virtual world, and you add elements of the physical environment to it, you get augmented virtuality (AV) (imagine being in a VR headset and seeing real-time video of the physical world through a portal).
          </p>
          <img class="proj-thumbnail center" src="images/RV-continuum-2.jpg" alt="Milgram and Kishino's Reality-Virtuality Continuum" max-width="100%"></img>
          <p class="break-above">
            In my research, I have investigated using AV and other techniques to improve interactions between people on opposite ends of the continuum, specifically focusing on when an immersed VR user is interrupted by a non-immersed person nearby in the user's physical environment.
            {{!-- ---that is, the interaction between an immersed VR user and a non-immersed interrupter nearby in the user's physical environment. --}}
            Additionally, I am exploring methods for transitioning augmented reality (AR) users' perspectives in a collaborative context, and how users might interact before and after perspective transitions.
            {{!-- These transitions are examples of crossing realities as well. --}}
            Each AR user's view on the world is their reality, so these transitions are examples of crossing realities as well.
          </p>
          <h5>
            Some of my research projects in this area:
          </h5>
        </div>
      </div>

      <div class="row proj-summary-group">
        <div class="col-3 proj-image-group">
          <div class="col-xs-12">
            <img class="proj-thumbnail" src="images/in-vr-CRI/_baseline_annotated_2b.png" width="100%"></img>
            <p class="thumbnail-caption">Baseline: no virtual avatar</p>
          </div>
          <div class="col-xs-12 inVRimages hideimage">
            <img class="proj-thumbnail" src="images/in-vr-CRI/_passthrough_annotated.png" width="100%"></img>
            <p class="thumbnail-caption">UI notification + passthrough view</p>
          </div>
          <div class="col-xs-12 inVRimages hideimage">
            <img class="proj-thumbnail" src="images/in-vr-CRI/_nda_2.png" width="100%"></img>
            <p class="thumbnail-caption">Non-diegetic avatar</p>
          </div>
          <div class="col-xs-12 inVRimages hideimage">
            <img class="proj-thumbnail" src="images/in-vr-CRI/_pda_2.png" width="100%"></img>
            <p class="thumbnail-caption">Partially diegetic avatar</p>
          </div>
          <div class="col-xs-12 inVRimages hideimage">
            <img class="proj-thumbnail" src="images/in-vr-CRI/_fda_2.png" width="100%"></img>
            <p class="thumbnail-caption">Fully diegetic avatar</p>
          </div>
        </div>
        {{!-- <div class="col-xs-9 col-md-9 proj-group-details"> --}}
        <div class="col-9 proj-group-details">
          <details id="inVRdetails">
          <summary>
              <span class="summary-title">
                InVR Interruptions: Toward Seamless Cross-Reality Interruptions of VR Users with Diegetic Representations of Interrupters
              </span>
              {{!-- <h4 class="">
              </h4> --}}
              <h6 class="subtitle subtitle-break">
                {{!-- Studying usability challenges caused by the isolating nature of VR --}}
                VR headsets block users' view of the physical environment. How should an interrupter be brought into the VR user's virtual world when needing to interact with them?
              </h6>
              {{!-- <div class="row subtitle subtitle-break">
                <div class="title-bar-tag">completed</div> <div class="title-bar-tag">research</div> <div class="title-bar-tag">XR</div>
              </div> --}}
          </summary>
          <div class="card card-body infoItem--card">
            <p>
              [ <a href="https://sreal.ucf.edu/wp-content/uploads/2021/08/ISMAR_2021_Paper__Interruptions_.pdf">Paper PDF</a> ]
              [ <a href="https://ieeexplore.ieee.org/abstract/document/9583786">IEEE Xplore</a> ]
              [ <a href="https://youtu.be/ioq4Q-ffNwE?t=1222">Presentation</a> ]  
              [ <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lHM2NP4AAAAJ&citation_for_view=lHM2NP4AAAAJ:d1gkVwhDpl0C">Google Scholar</a> ]
            </p>
            <p>
              When someone puts on a virtual reality headset, they are completely isolated from their physical environment, including the people around them. This is by design&mdash;VR presents to be the most immersive computing technology. However, there are many cases in which a person wants or needs to interact with someone immersed in VR. Some examples, where "you" are wearing a VR head-mounted display:
            </p>
            <ul>
              <li>You are playing a VR game in your home, and your roommate needs to ask what kind of pizza you want.</li>
              <li>You are working on a 3D design task in VR, and your collaborator needs to tell you about a specification update.</li>
              <li>You are passing time on a plane in peaceful virtual environment, and your seat neighbor needs to move around you to use the onboard lavatory.</li>
            </ul>
            <p>
              With the current state of the art, the interrupter cannot fully interact with the VR user unless they take off the headset. There is a lot of friction involved in that process, so it seems that there should be a communication channel that does not require the user to doff the headset. Additionally, the interruption is often jarring for the user. They are immersed in another world. When someone taps them on the shoulder or speaks to them, their physical environment abruptly calls them back. This process could be smoother.
            </p>
            <p>
              With the help of some awesome people in my lab, I designed, implemented, and ran an experimental design human subjects study to examine ways to facilitate this interaction. Our work was published in the proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR) in October 2021. You can read the full paper here: <a href="https://sreal.ucf.edu/wp-content/uploads/2021/08/ISMAR_2021_Paper__Interruptions_.pdf">Diegetic Representations for Seamless Cross-Reality Interruptions</a>.
            </p>
            <p>
              The word "diegetic" comes from describing narrative media elements. A story element is diegetic if it comes from within the context of the story world itself. For example, a sound in a movie is diegetic if it is produced by something in the scene (e.g., a radio playing a song). A different sound is non-diegetic if it is added to the scene as a narrative element (e.g., a musical score added to a scene where there is no orchestra plausibly nearby). I made a <a href="https://youtu.be/ioq4Q-ffNwE?t=1400">2-minute creative explanation for this concept</a> in my video presentation of this paper. You can watch the whole "live" presentation from that link if you want; my presentation starts around the <a href="https://youtu.be/ioq4Q-ffNwE?t=1222">20:20 mark</a>.
            </p>
            <p>
              VR complicates the common definition diegetic because the virtual world completely surrounds the user. We can talk about audio mostly in the same diegetic dimensions, but the lines between diegetic and non-diegetic visuals blur a little. We explored how one might vary the diegetic degree of the appearance of an avatar to represent a non-VR interrupter to a VR user, and the different effects that might have on the VR user's virtual experience and cross-reality interaction experience.
            </p>
            <p>
              I built a virtual office environment and tasked participants with stacking virtual blocks in a couple different formations&mdash;just an easy task that could help them fall into a rhythm pretty quickly. The experimenter interrupted them part-way through each block formation. The way the interrupter was represented to the VR user changed each time:
              <ul>
                <li><b>Baseline:</b> interrupter taps the user on their shoulder, user takes off the headset to interact with interrupter.</li>
                <li><b>UI + passthrough view:</b> UI notification alerted the user someone nearby wanted to speak with them, then the user activated the headset's passthrough view to see the interrupter through the headset's external cameras.</li>
                <li><b>Non-diegetic avatar:</b> The interrupter was represented in the virtual environment as a floating green sphere.</li>
                <li><b>Partially diegetic avatar:</b> The interrupter was represented in the virtual environment as an avatar in business clothes with a glowing green outline.</li>
                <li><b>Fully diegetic avatar:</b> The interrupter was represented in the virtual environment as an avatar in business clothes.</li>
              </ul>
            </p>
            <p>
              Based on a Cross-Reality Interaction Experience questionnaire we wrote, we found that participants rated the interaction experience with the interrupter highest for the partially and fully diegetic avatars. We also found that these avatars afforded a reasonably high sense of co-presence with the real-world interrupters, i.e., participants felt they were with a real person as opposed to a purely digital one. We found that participants more often preferred the partially diegetic representations. Their qualitative responses suggest why: several stated that the green outline helped them distinguish the avatar from the rest of the virtual environment; the outline suggested the avatar was not just an NPC (non-player character in a video game). I am interested in further exploring methods for representing cross-reality interactors, especially for interactions that may occur for longer periods (as opposed to brief interruptions).
            </p>
            <p>
              Additionally, we asked participants about their <em>place illusion</em>, or their sense of "being there" in the virtual environment before, during, and after the interruptions. We found that the avatar conditions led participants to experience a consistent and high sense of place illusion throughout the interruption, where the conditions that caused participants to take the headset off led to a drop in place illusion that did not recover immediately after the interruption. I am interested in investigating further how VR users' senses of presence move and change throughout a VR experience.
            </p>
          </div>
        </details>
        </div>
      </div>

      <div class="row proj-summary-group">
        <div class="col-3 proj-image-group">
          <div class="col-xs-12">
            <img class="proj-thumbnail" src="images/interrupting-vr/virtual_activity_cues_green.png"></img>
            <p class="thumbnail-caption">Green: Low Virtual Activity</p>
          </div>
          <div class="col-xs-12 interruptingVRimages hideimage">
            <img class="proj-thumbnail" src="images/interrupting-vr/virtual_activity_cues_yellow.png" width="100%"></img>
            <p class="thumbnail-caption">Yellow: Medium Virtual Activity</p>
          </div>
          <div class="col-xs-12 interruptingVRimages hideimage">
            <img class="proj-thumbnail" src="images/interrupting-vr/virtual_activity_cues_red.png" width="100%"></img>
            <p class="thumbnail-caption">Red: High Virtual Activity</p>
          </div>
        </div>
        <div class="col-9 proj-group-details">
          <details id="interruptingVRdetails">
          <summary>
              <span class="summary-title">
                Interrupting VR Users: Exploring Cues and Signaling to Improve Cross-Reality Interruptions
              </span>
              {{!-- <h4 class="">
              </h4> --}}
              <h6 class="subtitle subtitle-break">
                {{!-- Studying usability challenges caused by the isolating nature of VR --}}
                VR headsets block interrupters' ability to make judgments about VR users' mental state that they typically use to decide when to interrupt. What information should be displayed to interrupters, and how should it be displayed to them?
              </h6>
              {{!-- <div class="row subtitle subtitle-break">
                <div class="title-bar-tag">completed</div> <div class="title-bar-tag">research</div> <div class="title-bar-tag">XR</div>
              </div> --}}
          </summary>
          <div class="card card-body infoItem--card">
            <p>
              [ <a href="https://sreal.ucf.edu/wp-content/uploads/2022/10/ISMAR2022_Workshop_on_Prototyping_Cross_Reality_Systems.pdf">Paper PDF</a> ]
              {{!-- [ <a href="https://ieeexplore.ieee.org/abstract/document/9583786">IEEE Xplore</a> ]
              [ <a href="https://youtu.be/ioq4Q-ffNwE?t=1222">Presentation</a> ]  
              [ <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lHM2NP4AAAAJ&citation_for_view=lHM2NP4AAAAJ:d1gkVwhDpl0C">Google Scholar</a> ] --}}
            </p>
            <p>
              When someone we want to interact with is wearing an HMD, our usual ability to attribute mental states (e.g., busyness, engagement, openness) to the other person is inhibited. 
              We perform this attribution quite often in social interactions&mdash;in psychology it is referred to as developing a <em>theory of mind</em> about the other person.
              VR makes this process difficult because the VR headset blocks much of the user's face and does not share any information about their current task.
              This presents a problem for people in the VR user's physical environment who need to interact with (i.e., interrupt) the user.
              The interrupter does not have access to the cues they usually use to determine when and how to interrupt, nor the methods they traditionally use to signal an interruption.
              For example, they cannot read the VR user's full facial expression to determine how engaged they are with their task, and they cannot announce their intention to interrupt by slowly approaching.
              This disconnect is a problem that will become increasingly common as VR is used by more people, in more settings (e.g., the workplace), for more purposes, and for longer durations.
            </p>
            <p>
              To improve this interaction from the interrupter's perspective, I am designing and implementing both hardware and software prototypes, and running user studies to test them.
              I aim to produce simple modifications to VR headsets that restore these broken communication links and improve these interactions for all parties.
              To this end, I have explored attaching visual cues to a headset to communicate the VR user's virtual activity level (i.e., their engagement with their virtual task).
              I have also explored signaling mechanisms such as a gesture sensor mounted on a headset to allow an interrupter to initiate an interruption through a wave gesture.
            </p>
            <p>
              I presented a <a href="https://sreal.ucf.edu/wp-content/uploads/2022/09/ISMAR22_CrossReality_camready_3.pdf">poster</a> at IEEE ISMAR 2022 about an initial prototype and pilot study for this project.
              Through quantitative and qualitative data analysis, the pilot study suggested that the Virtual Activity Cues were helpful in providing interrupters with information about the VR user's mental states and useful for deciding when to interrupt.
              The gesture system did not seem to be helpful.
              Going forward, I am planning a user study to understand the information essential for interrupters to understand the VR user's mental states and interruptibility.
              Then, I will iterate on the design of this prototype to home in on cues that are most useful for communicating when and how one should interrupt.
            </p>
          </div>
        </details>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="content-section">
  <div class="container">
    <div class="title-bar-index pad-content">
      <h2>
        recent collaborators
      </h2>
      <p>
        <ul>
          <li>Robbe Cools (KU Leuven)</li>
          <li>Dr. Steven Feiner (CGUI Lab, Columbia University)</li>
          <li>Dr. Pamela Wisniewski (Vanderbilit University)</li>
          <li>Dr. Stephanie Carnell (VAR Lab, University of Central Florida)</li>
          <li>Dr. Nahal Norouzi (Meta Reality Labs)</li>
          <li>Raiffa Syamil (VAR Lab, University of Central Florida)</li>
          <li>Hiroshi Furuya (SREAL, University of Central Florida)</li>
          <li>Zubin Choudhary (SREAL, University of Central Florida)</li>
        </ul>
      </p>
    </div>
  </div>
</section>

<!-- Map Section -->
<div class="container map-container">
  {{!-- <iframe width="100%" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" src="https://www.openstreetmap.org/export/embed.html?bbox=-82.28485107421876%2C27.97499795326776%2C-80.48309326171876%2C29.099376992628493&amp;layer=mapnik&amp;marker=28.53868739844257%2C-81.38397216796875" style="border: 1px solid black"></iframe><br/><small><a href="https://www.openstreetmap.org/?mlat=28.5387&amp;mlon=-81.3840#map=10/28.5387/-81.3840">View Larger Map</a></small> --}}
  <div id="map"></div>
</div>

<!-- Acknowledgments section  -->
<section id="acknowledgments" class="content-section">
  <div class="container">
    <h1 style="padding-bottom: 0.2em;">acknowledgments</h1>
    <p>These are some of the things I used to build this space:</p>
    <ul class="ul-arrow">
      <li class="li-arrow">
        Reality-Virtuality Continuum image from: Milgram P, Drascic D. Perceptual Effects in Aligning Virtual and Real Objects in Augmented Reality Displays. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 1997;41(2):1239-1243. doi:10.1177/1071181397041002115
      </li>
      <li class="li-arrow"><a href="https://thomaspark.co/">Thomas Park's</a><a href="https://bootswatch.com/slate/"> Bootswatch theme</a>. I use the <a href="https://getbootstrap.com/">Bootstrap library</a> to make some of the styling and basic web interactions easier.</li>
      <li class="li-arrow">Glitched images generated by <a href="https://github.com/snorpey">Georg Fischer</a>'s <a href="https://snorpey.github.io/jpg-glitch/">Image Glitch Tool</a></li>
      <li class="li-arrow">Map markers: Font Awesome by Dave Gandy - <a rel="nofollow" class="external free" href="https://fortawesome.github.com/Font-Awesome">https://fortawesome.github.com/Font-Awesome</a> [<a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>]</li>
      <li class="li-arrow"><a href="https://expressjs.com/">Express Framework</a>. This gives me a nice and straightforward scaffold with which I organize my backend work.</li>
      <li class="li-arrow"><a href="https://nodejs.org/">Node.js</a>. This runs the site's backend.</li>
      <li class="li-arrow"><a href="https://aws.amazon.com/lightsail/">Amazon Lightsail</a>. This hosts all of the above.</li>
    </ul>
  </div>
</section>


<!-- Contact Section -->
<section id="contact" class="content-section">
  <div class="container">
    <h1 style="padding-bottom: 0.2em;">contact me</h1>
    <p>e-mail me at <span class="subtitle" style="padding-left: 0.25em; padding-right:0.15em;">matt [dot] gottsacker [at] gmail [dot] com</span>. Feedback and collaboration ideas are welcome.
    </p>
    <br>

</div>
</section>

<section id="user-data" class="content-section">
  <div class="container">
    <h1 style="padding-bottom: 0.2em;">user data</h1>
    <p>
      The only <a href="https://en.wikipedia.org/wiki/HTTP_cookie">cookie</a> used for this website checks whether you have already seen the banner about user data. If you have seen it, you won't see it again (unless you clear your cookies). 
    </p>
    <p>
      I use the free and open source <a href="https://www.goatcounter.com/">GoatCounter</a> for web analytics. I don't store any data on you, but I like seeing from where in the world people visit this webspace. In fact, you can see everything that I can see by visiting this website's <a href="https://mgspace.goatcounter.com/">public stats site</a>.
    </p>
  </div>
</section>

<!-- Bootstrap core JavaScript -->
{{!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script> --}}
<!-- <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script> -->

<script src="javascripts/bootstrap.min.js"></script>
<!-- Custom scripts for this page -->
<script src="javascripts/index.js"></script>
